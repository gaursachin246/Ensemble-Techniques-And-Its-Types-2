{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3492ee-0b6c-47f6-854a-eb8c322ecf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1. How does bagging reduce overfitting in decision trees?\n",
    "\n",
    "#1. Averaging out noise: By training multiple decision trees on different bootstrap samples, bagging averages out the noise and outliers in the data, reducing the impact of overfitting.\n",
    "\n",
    "#2. Reducing variance: Bagging combines the predictions of multiple decision trees, reducing the variance of the predictions and making the model more robust.\n",
    "\n",
    "#3. Smoothing the decision boundary: By averaging the predictions of multiple decision trees, bagging smooths the decision boundary, making it less sensitive to small changes in the data.\n",
    "\n",
    "#4. Avoiding over-specialization: Bagging prevents individual decision trees from over-specializing to a particular subset of the data, reducing overfitting.\n",
    "\n",
    "#5. Increasing model diversity: By training multiple decision trees on different bootstrap samples, bagging increases model diversity, reducing the correlation between models and making the ensemble more robust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7681a06-8ea9-4adb-a6ba-c2ad7f18eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "\n",
    "## 1. Increased diversity: Using different types of base learners can increase model diversity, leading to a more robust ensemble.\n",
    "#2. Improved accuracy: Combining different types of base learners can improve overall accuracy by leveraging their strengths.\n",
    "#3. Reduced overfitting: Using different types of base learners can reduce overfitting by averaging out their errors.\n",
    "\n",
    "#Disadvantages of using different types of base learners in bagging:\n",
    "\n",
    "#1. Increased complexity: Using different types of base learners can increase the complexity of the ensemble, making it harder to interpret.\n",
    "#2. Higher computational cost: Training multiple types of base learners can increase computational cost and training time.\n",
    "#3. Hyperparameter tuning: Using different types of base learners requires tuning multiple sets of hyperparameters, which can be challenging.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4ac746-1091-4441-a2c6-28123c8aaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "\n",
    "#- Aprendices base con un alto sesgo (por ejemplo, modelos lineales) tienden a producir un sesgo alto en el ensamblaje.\n",
    "#    - Aprendices base con un bajo sesgo (por ejemplo, árboles de decisión) tienden a producir un sesgo bajo en el ensamblaje.\n",
    "#- Varianza:\n",
    "#    - Aprendices base con una alta varianza (por ejemplo, árboles de decisión) tienden a producir una varianza alta en el ensamblaje.\n",
    "#    - Aprendices base con una baja varianza (por ejemplo, modelos lineales) tienden a producir una varianza baja en el ensamblaje.\n",
    "#\n",
    "#Al elegir un aprendiz base, debes equilibrar el sesgo y la varianza:\n",
    "\n",
    "#- Baja varianza y alto sesgo: El ensamblaje puede producir un rendimiento estable pero subóptimo.\n",
    "#- Alta varianza y bajo sesgo: El ensamblaje puede producir un rendimiento óptimo pero inestable.\n",
    "#- Equilibrio entre sesgo y varianza: El ensamblaje puede producir un rendimiento óptimo y estable.\n",
    "\n",
    "#Para lograr un buen equilibrio entre sesgo y varianza, puedes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a044e-c196-4296-87f9-16866111f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "\n",
    "#1. Voting: Each base learner predicts a class label.\n",
    "2. Majority voting: The final prediction is the class label with the most votes.\n",
    "\n",
    "Regression:\n",
    "\n",
    "1. Averaging: Each base learner predicts a continuous value.\n",
    "2. Average prediction: The final prediction is the average of the predicted values.\n",
    "\n",
    "In classification, bagging can help reduce overfitting by averaging out the noise in the class labels. In regression, bagging can help reduce overfitting by averaging out the noise in the predicted values.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
